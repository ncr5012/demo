{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waylLh9uQHR2",
        "outputId": "46c2a4b6-1195-4169-a667-25136b73d3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvJxknsWQfM6",
        "outputId": "df822b58-d6fb-48e8-8b0b-67c141dabd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "        Year  Month  Day  Hour  Minute   QQQ Open  QQQ Volume   SPY Open  \\\n",
            "0       2008      2   29    16       0  37.452999   1748021.0  97.830002   \n",
            "1       2008      2   29    15      59  37.462002   5252612.0  97.940002   \n",
            "2       2008      2   29    15      58  37.495998   4343744.0  97.926003   \n",
            "3       2008      2   29    15      57  37.444000   4551546.0  97.786003   \n",
            "4       2008      2   29    15      56  37.409000   5352330.0  97.771004   \n",
            "...      ...    ...  ...   ...     ...        ...         ...        ...   \n",
            "391852  2004      2    2     9      34  31.857000    413200.0  77.601997   \n",
            "391853  2004      2    2     9      33  31.849001    259300.0  77.601997   \n",
            "391854  2004      2    2     9      32  31.823000    215100.0  77.595001   \n",
            "391855  2004      2    2     9      31  31.813999     95900.0  77.560997   \n",
            "391856  2004      2    2     9      30  31.823000    650300.0  77.622002   \n",
            "\n",
            "        SPY Volume  UCO Open  ...   AXP Open  AXP Volume   WBA Open  \\\n",
            "0        4581913.0       0.0  ...  32.681999      9700.0  23.202000   \n",
            "1        2341487.0       0.0  ...  32.681999    181500.0  23.188999   \n",
            "2        2173619.0       0.0  ...  32.705002    185796.0  23.195000   \n",
            "3        1410278.0       0.0  ...  32.643002     91300.0  23.170000   \n",
            "4        1622486.0       0.0  ...  32.659000     94200.0  23.145000   \n",
            "...            ...       ...  ...        ...         ...        ...   \n",
            "391852     64900.0       0.0  ...  33.859001      7000.0  21.129000   \n",
            "391853     53700.0       0.0  ...  33.813999     44800.0  21.122999   \n",
            "391854     23800.0       0.0  ...  33.820000     66600.0  21.093000   \n",
            "391855     27100.0       0.0  ...  33.820000     18400.0  21.099001   \n",
            "391856    246800.0       0.0  ...  33.820000     96900.0  21.044001   \n",
            "\n",
            "        WBA Volume    HD Open  HD Volume  GME Open  GME Volume    BA Open  \\\n",
            "0           2200.0  17.728001    94700.0     7.141      6000.0  60.981998   \n",
            "1          55200.0  17.735001   210520.0     7.136    167200.0  60.952999   \n",
            "2          64600.0  17.747999   192046.0     7.138    157200.0  61.166000   \n",
            "3          46300.0  17.701000   128930.0     7.133     96588.0  61.099998   \n",
            "4          56000.0  17.695000   120744.0     7.135     97200.0  61.106998   \n",
            "...            ...        ...        ...       ...         ...        ...   \n",
            "391852      1000.0  22.184000     1800.0     1.432    224800.0  28.837999   \n",
            "391853      3800.0  22.197001    18700.0     0.000         0.0  28.789000   \n",
            "391854      4900.0  22.184000     1500.0     0.000         0.0  28.761999   \n",
            "391855     18000.0  22.134001    13400.0     0.000         0.0  28.707001   \n",
            "391856     57100.0  22.202999   183400.0     0.000         0.0  28.707001   \n",
            "\n",
            "        BA Volume  \n",
            "0          3800.0  \n",
            "1        178439.0  \n",
            "2        278200.0  \n",
            "3         73790.0  \n",
            "4         26100.0  \n",
            "...           ...  \n",
            "391852     4000.0  \n",
            "391853     7100.0  \n",
            "391854     1500.0  \n",
            "391855    10200.0  \n",
            "391856    50200.0  \n",
            "\n",
            "[391857 rows x 143 columns]\n",
            "        QQQ Open  QQQ Volume  SPY Open  SPY Volume  UCO Open  UCO Volume  \\\n",
            "0       0.414694    4.874831  0.537889   14.024452       0.0         0.0   \n",
            "1       0.416744   16.303005  0.547689    6.854913       0.0         0.0   \n",
            "2       0.424487   13.339264  0.546441    6.317721       0.0         0.0   \n",
            "3       0.412644   14.016889  0.533970    3.874970       0.0         0.0   \n",
            "4       0.404673   16.628178  0.532634    4.554052       0.0         0.0   \n",
            "...          ...         ...       ...         ...       ...         ...   \n",
            "391852 -0.859810    0.522092 -1.264097   -0.430346       0.0         0.0   \n",
            "391853 -0.861632    0.020237 -1.264097   -0.466187       0.0         0.0   \n",
            "391854 -0.867554   -0.123895 -1.264721   -0.561869       0.0         0.0   \n",
            "391855 -0.869604   -0.512596 -1.267750   -0.551309       0.0         0.0   \n",
            "391856 -0.867554    1.295255 -1.262315    0.151748       0.0         0.0   \n",
            "\n",
            "        CORN Open  CORN Volume  WEAT Open  WEAT Volume  ...  HD Volume  \\\n",
            "0             0.0          0.0        0.0          0.0  ...   1.222411   \n",
            "1             0.0          0.0        0.0          0.0  ...   3.328082   \n",
            "2             0.0          0.0        0.0          0.0  ...   2.992214   \n",
            "3             0.0          0.0        0.0          0.0  ...   1.844731   \n",
            "4             0.0          0.0        0.0          0.0  ...   1.695905   \n",
            "...           ...          ...        ...          ...  ...        ...   \n",
            "391852        0.0          0.0        0.0          0.0  ...  -0.466562   \n",
            "391853        0.0          0.0        0.0          0.0  ...  -0.159311   \n",
            "391854        0.0          0.0        0.0          0.0  ...  -0.472016   \n",
            "391855        0.0          0.0        0.0          0.0  ...  -0.255668   \n",
            "391856        0.0          0.0        0.0          0.0  ...   2.835025   \n",
            "\n",
            "        GME Open  GME Volume   BA Open  BA Volume  Year  Month  Day  Hour  \\\n",
            "0       1.280407   -0.335739  0.661030  -0.356373  2008      2   29    16   \n",
            "1       1.278389    3.437037  0.658914   9.527073  2008      2   29    15   \n",
            "2       1.279196    3.202994  0.674453  15.172904  2008      2   29    15   \n",
            "3       1.277177    1.784411  0.669638   3.604612  2008      2   29    15   \n",
            "4       1.277985    1.798735  0.670149   0.905664  2008      2   29    15   \n",
            "...          ...         ...       ...        ...   ...    ...  ...   ...   \n",
            "391852 -1.024637    4.785126 -1.684016  -0.345054  2004      2    2     9   \n",
            "391853 -1.602816   -0.476165 -1.687591  -0.169614  2004      2    2     9   \n",
            "391854 -1.602816   -0.476165 -1.689561  -0.486538  2004      2    2     9   \n",
            "391855 -1.602816   -0.476165 -1.693573   0.005826  2004      2    2     9   \n",
            "391856 -1.602816   -0.476165 -1.693573   2.269569  2004      2    2     9   \n",
            "\n",
            "        Minute  \n",
            "0            0  \n",
            "1           59  \n",
            "2           58  \n",
            "3           57  \n",
            "4           56  \n",
            "...        ...  \n",
            "391852      34  \n",
            "391853      33  \n",
            "391854      32  \n",
            "391855      31  \n",
            "391856      30  \n",
            "\n",
            "[391857 rows x 143 columns]\n",
            "         QQQ Open    QQQ Volume   SPY Open    SPY Volume  UCO Open  \\\n",
            "0       37.452999  1.748021e+06  97.830002  4.581913e+06       0.0   \n",
            "1       37.462002  5.252612e+06  97.940002  2.341487e+06       0.0   \n",
            "2       37.495998  4.343744e+06  97.926003  2.173619e+06       0.0   \n",
            "3       37.444000  4.551546e+06  97.786003  1.410278e+06       0.0   \n",
            "4       37.409000  5.352330e+06  97.771004  1.622486e+06       0.0   \n",
            "...           ...           ...        ...           ...       ...   \n",
            "391852  31.857000  4.132000e+05  77.601997  6.490000e+04       0.0   \n",
            "391853  31.849001  2.593000e+05  77.601997  5.370000e+04       0.0   \n",
            "391854  31.823000  2.151000e+05  77.595001  2.380000e+04       0.0   \n",
            "391855  31.813999  9.590001e+04  77.560997  2.710000e+04       0.0   \n",
            "391856  31.823000  6.503000e+05  77.622002  2.468000e+05       0.0   \n",
            "\n",
            "        UCO Volume  CORN Open  CORN Volume  WEAT Open  WEAT Volume  ...  \\\n",
            "0              0.0        0.0          0.0        0.0          0.0  ...   \n",
            "1              0.0        0.0          0.0        0.0          0.0  ...   \n",
            "2              0.0        0.0          0.0        0.0          0.0  ...   \n",
            "3              0.0        0.0          0.0        0.0          0.0  ...   \n",
            "4              0.0        0.0          0.0        0.0          0.0  ...   \n",
            "...            ...        ...          ...        ...          ...  ...   \n",
            "391852         0.0        0.0          0.0        0.0          0.0  ...   \n",
            "391853         0.0        0.0          0.0        0.0          0.0  ...   \n",
            "391854         0.0        0.0          0.0        0.0          0.0  ...   \n",
            "391855         0.0        0.0          0.0        0.0          0.0  ...   \n",
            "391856         0.0        0.0          0.0        0.0          0.0  ...   \n",
            "\n",
            "            HD Volume      GME Open     GME Volume    BA Open      BA Volume  \\\n",
            "0        94700.000000  7.141000e+00    5999.999512  60.981998    3800.000000   \n",
            "1       210520.000000  7.136000e+00  167200.000000  60.952999  178438.984375   \n",
            "2       192046.000000  7.138000e+00  157200.000000  61.166000  278200.000000   \n",
            "3       128930.000000  7.133000e+00   96588.000000  61.099998   73790.000000   \n",
            "4       120744.000000  7.135000e+00   97200.000000  61.106998   26100.000000   \n",
            "...               ...           ...            ...        ...            ...   \n",
            "391852    1799.999023  1.432000e+00  224800.000000  28.837999    4000.000000   \n",
            "391853   18700.000000  6.439164e-08      -0.000461  28.789000    7100.000000   \n",
            "391854    1499.999023  6.439164e-08      -0.000461  28.761999    1499.999512   \n",
            "391855   13400.000000  6.439164e-08      -0.000461  28.707001   10200.000000   \n",
            "391856  183400.000000  6.439164e-08      -0.000461  28.707001   50200.000000   \n",
            "\n",
            "        Year  Month  Day  Hour  Minute  \n",
            "0       2008      2   29    16       0  \n",
            "1       2008      2   29    15      59  \n",
            "2       2008      2   29    15      58  \n",
            "3       2008      2   29    15      57  \n",
            "4       2008      2   29    15      56  \n",
            "...      ...    ...  ...   ...     ...  \n",
            "391852  2004      2    2     9      34  \n",
            "391853  2004      2    2     9      33  \n",
            "391854  2004      2    2     9      32  \n",
            "391855  2004      2    2     9      31  \n",
            "391856  2004      2    2     9      30  \n",
            "\n",
            "[391857 rows x 143 columns]\n",
            "6087\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "torch.set_printoptions(sci_mode=False, precision=4, threshold=5000)\n",
        "\n",
        "num_layers = 6 #Not tuned\n",
        "num_heads = 8 #Not tuned\n",
        "dim_feedforward = 512 # Not tuned\n",
        "input_sequence_length = 2340 #6-day lookback window (assuming non-extended hour data) to ensure a friday can always see price action from the previous friday\n",
        "prediction_length = 1 #Predict the next minute\n",
        "NUM_WORKERS = 2 #Not Tuned\n",
        "BATCH_SIZE = 64 #Not Tuned - change to 1 for inference code at the bottom\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "\n",
        "model_save_path = '/content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth'\n",
        "# Path where the model is saved\n",
        "model_load_path = '/content/drive/My Drive/Jackie_Net_Models/model_training_2008_2024_save.pth'\n",
        "\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Set torch print options for better readability\n",
        "torch.set_printoptions(sci_mode=False, precision=4, threshold=5000)\n",
        "\n",
        "# Function definitions\n",
        "def scale_data(df):\n",
        "    # Separate the datetime columns\n",
        "    datetime_columns = df[['Year', 'Month', 'Day', 'Hour', 'Minute']]\n",
        "    numerical_data = df.drop(columns=['Year', 'Month', 'Day', 'Hour', 'Minute'])\n",
        "\n",
        "    # Normalize the numerical data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_numerical_data = scaler.fit_transform(numerical_data)\n",
        "    scaled_numerical_data_df = pd.DataFrame(scaled_numerical_data, columns=numerical_data.columns)\n",
        "\n",
        "    # Concatenate the datetime columns back\n",
        "    df_scaled = pd.concat([scaled_numerical_data_df, datetime_columns], axis=1)\n",
        "\n",
        "    return df_scaled, scaler\n",
        "\n",
        "#Check function to inspect if descaling is applied correctly\n",
        "\n",
        "def descale_data(df_scaled, scaler):\n",
        "    # Assume the last 5 columns are datetime columns for simplicity\n",
        "    datetime_columns = df_scaled.iloc[:, -5:]\n",
        "    numerical_data_scaled = df_scaled.iloc[:, :-5]\n",
        "\n",
        "    # Inverse transform the numerical data\n",
        "    numerical_data_descaled = scaler.inverse_transform(numerical_data_scaled)\n",
        "    numerical_data_descaled_df = pd.DataFrame(numerical_data_descaled, columns=numerical_data_scaled.columns)\n",
        "\n",
        "    # Concatenate the datetime columns back\n",
        "    df_descaled = pd.concat([numerical_data_descaled_df, datetime_columns], axis=1)\n",
        "\n",
        "    return df_descaled\n",
        "\n",
        "def descale_tensor(tensor, scaler, column_names):\n",
        "    \"\"\"\n",
        "    Descales a tensor output from the model, reintegrates datetime columns, and reconstructs a DataFrame\n",
        "    with an identical structure as the original scaled DataFrame.\n",
        "\n",
        "    :param tensor: The tensor output from the model.\n",
        "    :param scaler: The scaler used to scale the data initially.\n",
        "    :param original_df_columns: The list of all column names from the original DataFrame.\n",
        "    :param datetime_columns_names: The list of datetime column names in the original DataFrame.\n",
        "    :return: A DataFrame with descaled data and a structure identical to the scaled DataFrame.\n",
        "    \"\"\"\n",
        "     # Ensure tensor is in CPU and convert it to numpy\n",
        "    print(f\"Original tensor shape: {tensor.shape}\")  # Debug: Check the shape before any operation\n",
        "    data_numpy = tensor.cpu().detach().numpy().reshape(1, -1)  # Reshape the tensor\n",
        "    print(f\"Reshaped array shape: {data_numpy.shape}\")  # Debug: Verify the shape after reshaping\n",
        "\n",
        "\n",
        "    # Separate the last 5 columns as datetime columns for simplicity\n",
        "    datetime_data = data_numpy[:,-5:]\n",
        "    numerical_data_scaled = data_numpy[:,:-5]\n",
        "\n",
        "     # Inverse transform the numerical data\n",
        "    numerical_data_descaled = scaler.inverse_transform(numerical_data_scaled)\n",
        "\n",
        "    # Concatenate the descaled numerical data with the datetime data\n",
        "    descaled_data = np.concatenate([numerical_data_descaled, datetime_data], axis=1)\n",
        "    # Assuming datetime data were stored or can be reconstructed similarly to how they were handled originally\n",
        "    # For simplicity, this example doesn't handle the reconstruction of datetime columns\n",
        "    # You would need to ensure you have this data available or reconstruct it as needed\n",
        "\n",
        "    # Create a DataFrame with the descaled data and original column names\n",
        "    df_descaled = pd.DataFrame(descaled_data, columns=column_names)\n",
        "\n",
        "    return df_descaled\n",
        "\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, data, input_sequence_length, prediction_length):\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            # Convert DataFrame to NumPy array\n",
        "            data = data.values\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.input_sequence_length = input_sequence_length\n",
        "        self.prediction_length = prediction_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.input_sequence_length - self.prediction_length + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Since data is in reverse chronological order, adjust indexing accordingly\n",
        "        # The target sequence immediately follows the input sequence in chronological order,\n",
        "        # which means it precedes in the dataset's order\n",
        "        x_start = index + self.prediction_length\n",
        "        x_end = x_start + self.input_sequence_length\n",
        "        y_start = index\n",
        "        y_end = index + self.prediction_length\n",
        "\n",
        "        x = self.data[x_start:x_end]\n",
        "        y = self.data[y_start:y_end]\n",
        "\n",
        "        # Reverse x to have it in chronological order for processing\n",
        "        x = x.flip(dims=[0])\n",
        "\n",
        "        return x, y.squeeze(), index  # Return the index as well\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Read the Parquet file\n",
        "path_transformer_input = '/content/drive/My Drive/Chrono_Jackie_Net/price_data_pt_2004_2008_pp.parquet'\n",
        "df = pd.read_parquet(path_transformer_input)\n",
        "print(df)\n",
        "\n",
        "# Scale the data\n",
        "df_scaled, scaler = scale_data(df)\n",
        "print(df_scaled)\n",
        "\n",
        "# Get the numerical column names from the original DataFrame, excluding the datetime columns\n",
        "column_names = df_scaled.columns.tolist()\n",
        "\n",
        "# Descale the data (for demonstration)\n",
        "df_descaled = descale_data(df_scaled, scaler)\n",
        "print(df_descaled)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = StockDataset(df_scaled, input_sequence_length, prediction_length)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "num_batches = len(dataloader)\n",
        "print(num_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCv9w7C3HPI9",
        "outputId": "0218d330-c52a-4478-ad0f-d147b689a554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "JackieNet(\n",
              "  (embedding): Linear(in_features=143, out_features=512, bias=True)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transformer_decoder): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=512, out_features=143, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "class JackieNet(nn.Module):\n",
        "    def __init__(self, num_features, num_layers, num_heads, dim_feedforward, prediction_length):\n",
        "        super(JackieNet, self).__init__()\n",
        "        self.embedding = nn.Linear(num_features, dim_feedforward)\n",
        "        #self.positional_encoding = PositionalEncoding(dim_feedforward, max_seq_length)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=dim_feedforward,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=dim_feedforward\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Initialize the Transformer Decoder\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model=dim_feedforward,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=dim_feedforward\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Final layer to output the prediction for the next timestep\n",
        "        self.final_layer = nn.Linear(dim_feedforward, prediction_length*num_features)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Embedding and encoding as before\n",
        "        # src shape: [sequence_length, batch_size, num_features]\n",
        "        src = self.embedding(src)\n",
        "        #src = src + self.positional_encoding(src)\n",
        "        memory = self.transformer_encoder(src)\n",
        "\n",
        "        # Initialize the decoder input with zeros\n",
        "        # Shape: [1, batch_size, num_features]\n",
        "        #decoder_input = torch.zeros(1, src.size(1), src.size(2), device=src.device)\n",
        "        decoder_input = src[-1, :, :].unsqueeze(0)\n",
        "\n",
        "        # Transformer Decoder: Process the decoder input\n",
        "        decoded_output = self.transformer_decoder(decoder_input, memory)\n",
        "\n",
        "        # Final Forecast: Take the output from the decoder\n",
        "        # Only take the first timestep (last dimension) from the decoded output\n",
        "        next_step_forecast = self.final_layer(decoded_output[0])\n",
        "\n",
        "        # Reshape to [1, batch_size, num_features]\n",
        "        output = next_step_forecast.unsqueeze(0)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a long enough positional encoding\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Change x to (batch_size, seq_len, features)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # Add positional encoding\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        # Change back to (seq_len, batch_size, features)\n",
        "        return x.permute(1, 0, 2)\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_progress(predictions, targets, epoch_losses, epoch):\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
        "\n",
        "    # Plot Predictions vs. Targets\n",
        "    axs[0].plot(predictions, label='Predictions')\n",
        "    axs[0].plot(targets, label='Targets')\n",
        "    axs[0].set_title(f'Epoch {epoch}: Predictions vs Targets')\n",
        "    axs[0].set_xlabel('Sample Index')\n",
        "    axs[0].set_ylabel('Value')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot Training Loss\n",
        "    axs[1].plot(range(1, epoch + 1), epoch_losses[:epoch], marker='o', linestyle='-')\n",
        "    axs[1].set_title('Training Loss Over Epochs')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xticks(range(1, epoch + 1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Model initialization\n",
        "num_features = df_scaled.shape[1]\n",
        "\n",
        "# Initialize the model\n",
        "model = JackieNet(num_features, num_layers, num_heads, dim_feedforward, prediction_length)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORSE_iFRQ0mm",
        "outputId": "465ea450-ce2f-4808-aa46-ca462d8e1ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/My Drive/Jackie_Net_Models/model_training_2008_2024_save.pth\n",
            "Epoch 1, 499 batches processed. Checkpoint_loss: 1723.5361771583557. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 3.453980314946605\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 998 batches processed. Checkpoint_loss: 1742.5525414943695. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 3.4920892615117625\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 1497 batches processed. Checkpoint_loss: 1635.1425139904022. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 3.2768387053915875\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 1996 batches processed. Checkpoint_loss: 1103.3105654716492. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 2.2110432173780543\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 2495 batches processed. Checkpoint_loss: 904.3532431125641. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.8123311485221725\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 2994 batches processed. Checkpoint_loss: 881.8262176513672. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.7671868089205756\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 3493 batches processed. Checkpoint_loss: 875.6319680213928. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.7547734830088033\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 3992 batches processed. Checkpoint_loss: 901.3479743003845. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.8063085657322335\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 4491 batches processed. Checkpoint_loss: 865.1194714307785. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.7337063555727024\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 4990 batches processed. Checkpoint_loss: 901.3182102441788. Checkpoint_batches: 499\n",
            "CheckpointAverage Loss: 1.8062489183250077\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n",
            "Epoch 1, 5488 batches processed. Checkpoint_loss: 879.9200789928436. Checkpoint_batches: 498\n",
            "CheckpointAverage Loss: 1.7669077891422562\n",
            "Model saved to /content/drive/My Drive/Jackie_Net_Models/model_training_2004_2024_save.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if the saved model exists and load it\n",
        "if os.path.isfile(model_load_path):\n",
        "    model.load_state_dict(torch.load(model_load_path))\n",
        "    print(f'Model loaded from {model_load_path}')\n",
        "else:\n",
        "    print('No saved model found, starting training from scratch.')\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    checkpoint_loss = 0\n",
        "    num_batches = 0\n",
        "    checkpoint_batches = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    start_time_epoch = time.time()  # Record the start time of the epoch\n",
        "    last_check = start_time_epoch  # Initialize the last check time\n",
        "\n",
        "    for input_sequence, target_sequence, start_index in dataloader:\n",
        "        # Transpose sequences to match expected shape: (seq_len, batch_size, features)\n",
        "        input_sequence = input_sequence.transpose(0, 1)  # Shape: (4800, batch_size, num_features)\n",
        "        #target_sequence = target_sequence.transpose(0, 1)  # Ensure target is correctly shaped\n",
        "\n",
        "        # Transfer to the device (GPU/CPU)\n",
        "        input_sequence = input_sequence.to(device)\n",
        "        target_sequence = target_sequence.to(device)\n",
        "\n",
        "\n",
        "        #print(f\"input_sequence.shape: {input_sequence.shape}\")\n",
        "        #print(f\"target_sequence.shape: {target_sequence.shape}\")\n",
        "        #print(f\"input_sequence: {input_sequence}\")\n",
        "        #print(f\"input_sequence[0]: {input_sequence[0]}\")\n",
        "        #print(f\"input_sequence[-1]: {input_sequence[-1]}\")\n",
        "        #print(f\"target_sequence: {target_sequence}\")\n",
        "\n",
        "\n",
        "        # Forward pass: The forward method of JackieNet takes only the input sequence\n",
        "        output = model(input_sequence)  # The model internally handles the start token for the decoder\n",
        "        #print(f\"input: {input_sequence.shape}\")\n",
        "        #print(f\"output: {output.shape}\")\n",
        "        #print(f\"tgt_sequence: {target_sequence.shape}\")\n",
        "        output_squeezed = torch.squeeze(output, 0)  # Removes the first dimension\n",
        "        #print(f\"output_squeezed: {output_squeezed.shape}\")\n",
        "\n",
        "        #print(f\"output: {output}\")\n",
        "\n",
        "        #output_descaled = descale_data(output, scaler)\n",
        "        #print(f\"output: {output_descaled}\")\n",
        "\n",
        "        # Compute loss: Compare the model's selected output with the actual selected target features\n",
        "        loss = loss_fn(output_squeezed, target_sequence)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        checkpoint_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        checkpoint_batches += 1\n",
        "\n",
        "        current_time = time.time()  # Get the current time\n",
        "        if current_time - last_check >= 900:  # Check if 5 minutes have passed 300\n",
        "            print(f'Epoch {epoch+1}, {num_batches} batches processed. Checkpoint_loss: {checkpoint_loss}. Checkpoint_batches: {checkpoint_batches}')\n",
        "            average_loss = checkpoint_loss / checkpoint_batches\n",
        "            print(f'CheckpointAverage Loss: {average_loss}')\n",
        "            checkpoint_loss = 0\n",
        "            checkpoint_batches = 0\n",
        "\n",
        "            last_check = current_time  # Update the last check time\n",
        "\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f'Model saved to {model_save_path}')\n",
        "\n",
        "\n",
        "    average_total_loss = total_loss / num_batches\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss}')\n",
        "\n",
        "    # Save the model to Google Drive\n",
        "    epoch_save_path = f'/content/drive/My Drive/Jackie_Net_Models/model_2004_2024_epoch_{epoch+1}.pth'\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f'Model saved to {epoch_save_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8ubdvGEiBqj",
        "outputId": "470f8e96-d444-4a76-d938-6805f95949cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 143])\n",
            "Original tensor shape: torch.Size([1, 143])\n",
            "Reshaped array shape: (1, 143)\n",
            "Original tensor shape: torch.Size([1, 143])\n",
            "Reshaped array shape: (1, 143)\n",
            "Original tensor shape: torch.Size([1, 143])\n",
            "Reshaped array shape: (1, 143)\n",
            "Descaled Input last minute:       QQQ Open  QQQ Volume    SPY Open  SPY Volume   UCO Open  UCO Volume  \\\n",
            "0  303.192993    114874.0  387.899994    107674.0  21.940001     14518.0   \n",
            "\n",
            "   CORN Open  CORN Volume  WEAT Open  WEAT Volume  ...    HD Volume  GME Open  \\\n",
            "0  24.712999        707.0      7.047   350.000122  ...  1661.999878     16.74   \n",
            "\n",
            "    GME Volume     BA Open   BA Volume    Year  Month   Day  Hour  Minute  \n",
            "0  4058.999023  203.789993  658.000061  2023.0    3.0  20.0  14.0    38.0  \n",
            "\n",
            "[1 rows x 143 columns]\n",
            "Descaled Output:       QQQ Open  QQQ Volume    SPY Open     SPY Volume   UCO Open    UCO Volume  \\\n",
            "0  306.891022     62755.5  385.774323  126847.265625  23.590811  21544.390625   \n",
            "\n",
            "   CORN Open  CORN Volume  WEAT Open  WEAT Volume  ...     HD Volume  \\\n",
            "0  21.278757  2272.545654   7.716595  4699.299316  ... -66590.296875   \n",
            "\n",
            "   GME Open    GME Volume     BA Open     BA Volume         Year    Month  \\\n",
            "0  24.51836  133405.78125  195.960373  27212.566406  2042.829102  5.92065   \n",
            "\n",
            "         Day       Hour     Minute  \n",
            "0  16.630835  12.237551  31.886816  \n",
            "\n",
            "[1 rows x 143 columns]\n",
            "Descaled Target:       QQQ Open  QQQ Volume    SPY Open  SPY Volume   UCO Open  UCO Volume  \\\n",
            "0  303.242004     37965.0  387.950012    234321.0  21.879999     15598.0   \n",
            "\n",
            "   CORN Open  CORN Volume  WEAT Open  WEAT Volume  ...    HD Volume  GME Open  \\\n",
            "0  24.712999        707.0      7.047   350.000122  ...  1421.999878     16.76   \n",
            "\n",
            "    GME Volume     BA Open  BA Volume    Year  Month   Day  Hour  Minute  \n",
            "0  4431.999023  203.839996     1306.0  2023.0    3.0  20.0  14.0    39.0  \n",
            "\n",
            "[1 rows x 143 columns]\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(model_load_path))\n",
        "\n",
        "# Prepare the model for inference\n",
        "model.eval()\n",
        "\n",
        "for input_sequence, target_sequence, start_index in dataloader:\n",
        "    break  # This will leave `input_sequence` containing the first batch\n",
        "\n",
        "# If your model expects the input in a specific shape, ensure to reshape it accordingly\n",
        "# For example, if it expects (seq_len, batch_size, num_features):\n",
        "input_sequence = input_sequence.transpose(0, 1)\n",
        "\n",
        "# Transfer to the device (GPU/CPU)\n",
        "input_sequence = input_sequence.to(device)\n",
        "target_sequence = target_sequence.to(device)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():  # No need to track gradients\n",
        "    output = model(input_sequence)\n",
        "\n",
        "# Process the output as needed (this step is highly application-specific)\n",
        "# For example, if you need to descale your output:\n",
        "# output_descaled = descale_data(output, scaler)\n",
        "\n",
        "print(output.view(1,-1).shape)  # Or any other post-processing\n",
        "#adjusted_output = output.squeeze()\n",
        "\n",
        "descaled_input_last_minute = descale_tensor(input_sequence[-1].view(1,-1), scaler, column_names)\n",
        "descaled_output = descale_tensor(output.view(1,-1), scaler, column_names)\n",
        "descaled_target = descale_tensor(target_sequence.view(1, -1), scaler, column_names)  # Reshape target_sequence similarly if needed\n",
        "\n",
        "print(\"Descaled Input last minute: \", descaled_input_last_minute)\n",
        "print(\"Descaled Output: \",descaled_output)\n",
        "print(\"Descaled Target: \", descaled_target)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}