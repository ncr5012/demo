{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTz63Nm8AmSW",
        "outputId": "33a8af7b-5eb8-4fe2-aa6b-ea512359f469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "#6/25/2021 - Roadmap - 1. build in playing untrained and trained agent functionality 2. stress test everything 3. send to compstrat professors / professor zaman if they are interested in using it in a class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n",
        "import gym\n",
        "from datetime import date\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as nn_utils\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "from gym import spaces, logger\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "from tensorboardX import SummaryWriter\n",
        "import tensorflow\n",
        "import datetime\n",
        "import collections\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4m-DHOn84YNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffcf855-774b-4fd5-ddf2-34c4540d5297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-4\n",
        "REPLAY_SIZE = 100000\n",
        "REPLAY_INITIAL = 10000\n",
        "REWARD_STEPS = 5\n",
        "EPSILON = 0.3\n",
        "NUM_ENVS = 30\n",
        "LEARNING_RATE_ACTOR = 0.001\n",
        "LEARNING_RATE_CRITIC = 0.001\n",
        "STOP_REWARD = 200000\n",
        "\n",
        "TEST_ITERS = 1000\n",
        "\n",
        "VMAX = 2000\n",
        "VMIN = 0\n",
        "N_ATOMS = 4001\n",
        "DELTA_Z = (VMAX - VMIN) / (N_ATOMS - 1)\n",
        "\n",
        "HID_SIZE = 64\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0OLhdxpzrhpB"
      },
      "outputs": [],
      "source": [
        "class RewardTracker:\n",
        "    def __init__(self, writer, stop_reward):\n",
        "        self.writer = writer\n",
        "        self.stop_reward = stop_reward\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.ts = time.time()\n",
        "        self.ts_frame = 0\n",
        "        self.total_rewardsA = []\n",
        "        self.total_rewardsB = []\n",
        "        self.total_rewardsC = []\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.writer.close()\n",
        "\n",
        "    def rewardA(self, rewardA, frame, epsilon=None):\n",
        "        self.total_rewardsA.append(rewardA)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_rewardA = np.mean(self.total_rewardsA[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        #print(\"%d: done %d games, Arthur's mean reward %.3f, Bonnie's mean reward %.3f, Clyde's mean reward %.3f, speed %.2f f/s%s\" % (\n",
        "            #frame, len(self.total_rewardsA), mean_rewardA, mean_rewardB, mean_rewardC, speed, epsilon_str\n",
        "        #))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"Arthur's reward_100\", mean_rewardA, frame)\n",
        "        self.writer.add_scalar(\"Arthur's reward\", rewardA, frame)\n",
        "\n",
        "        if mean_rewardA > self.stop_reward:\n",
        "            print(\"Arthur Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def rewardB(self, rewardB, frame, epsilon=None):\n",
        "\n",
        "        self.total_rewardsB.append(rewardB)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_rewardB = np.mean(self.total_rewardsB[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        #print(\"%d: done %d games, Arthur's mean reward %.3f, Bonnie's mean reward %.3f, Clyde's mean reward %.3f, speed %.2f f/s%s\" % (\n",
        "            #frame, len(self.total_rewardsA), mean_rewardA, mean_rewardB, mean_rewardC, speed, epsilon_str\n",
        "        #))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"Bonnie's reward_100\", mean_rewardB, frame)\n",
        "        self.writer.add_scalar(\"Bonie's reward\", rewardB, frame)\n",
        "\n",
        "        if mean_rewardB > self.stop_reward:\n",
        "            print(\"Bonnie Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def rewardC(self, rewardC, frame, epsilon=None):\n",
        "\n",
        "        self.total_rewardsC.append(rewardC)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_rewardC = np.mean(self.total_rewardsC[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        #print(\"%d: done %d games, Arthur's mean reward %.3f, Bonnie's mean reward %.3f, Clyde's mean reward %.3f, speed %.2f f/s%s\" % (\n",
        "            #frame, len(self.total_rewardsA), mean_rewardA, mean_rewardB, mean_rewardC, speed, epsilon_str\n",
        "        #))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"Clyde's reward_100\", mean_rewardC, frame)\n",
        "        self.writer.add_scalar(\"Clyde's reward\", rewardC, frame)\n",
        "\n",
        "        if mean_rewardC > self.stop_reward:\n",
        "            print(\"Clyde Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "class TBMeanTracker:\n",
        "    \"\"\"\n",
        "    TensorBoard value tracker: allows to batch fixed amount of historical values and write their mean into TB\n",
        "    Designed and tested with pytorch-tensorboard in mind\n",
        "    \"\"\"\n",
        "    def __init__(self, writer, batch_size):\n",
        "        \"\"\"\n",
        "        :param writer: writer with close() and add_scalar() methods\n",
        "        :param batch_size: integer size of batch to track\n",
        "        \"\"\"\n",
        "        assert isinstance(batch_size, int)\n",
        "        assert writer is not None\n",
        "        self.writer = writer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __enter__(self):\n",
        "        self._batches = collections.defaultdict(list)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.writer.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def _as_float(value):\n",
        "        assert isinstance(value, (float, int, np.ndarray, np.generic, torch.autograd.Variable)) or torch.is_tensor(value)\n",
        "        tensor_val = None\n",
        "        if isinstance(value, torch.autograd.Variable):\n",
        "            tensor_val = value.data\n",
        "        elif torch.is_tensor(value):\n",
        "            tensor_val = value\n",
        "\n",
        "        if tensor_val is not None:\n",
        "            return tensor_val.float().mean().item()\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            return float(np.mean(value))\n",
        "        else:\n",
        "            return float(value)\n",
        "\n",
        "    def track(self, param_name, value, iter_index):\n",
        "        assert isinstance(param_name, str)\n",
        "        assert isinstance(iter_index, int)\n",
        "\n",
        "        data = self._batches[param_name]\n",
        "        data.append(self._as_float(value))\n",
        "\n",
        "        if len(data) >= self.batch_size:\n",
        "            self.writer.add_scalar(param_name, np.mean(data), iter_index)\n",
        "            data.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tztVghDdAkHi"
      },
      "outputs": [],
      "source": [
        "class ShrimpEnv(gym.Env):\n",
        "\n",
        "     def __init__(self):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #self.action_space = gym.spaces.Box(low=0,high=100, shape=(1,), dtype=np.float32)\n",
        "            #self.action_space = gym.spaces.Discrete(100)\n",
        "            self.action_space = gym.spaces.Discrete(1)\n",
        "            self.observation_space = gym.spaces.Box(low=0, high=100,shape=(4,), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "            self.state = None\n",
        "            self.reward = None\n",
        "            self.steps_left = None\n",
        "            self.done = False\n",
        "\n",
        "\n",
        "     def step(self, actionA, actionB, actionC):\n",
        "\n",
        "            self.steps_left -= 1\n",
        "\n",
        "\n",
        "\n",
        "            done = bool(\n",
        "                self.steps_left == 1\n",
        "            )\n",
        "            #print(\"action A\", actionA)\n",
        "            actionAs = (actionA[0]*29.3)+50\n",
        "            actionBs = (actionB[0]*29.3)+50\n",
        "            actionCs = (actionC[0]*29.3)+50\n",
        "\n",
        "            self.raw_price = 100-(0.5*(actionAs+actionBs+actionCs))\n",
        "\n",
        "            if self.raw_price <= 0:\n",
        "\n",
        "              self.todays_price = 0\n",
        "\n",
        "            else:\n",
        "\n",
        "              self.todays_price = self.raw_price\n",
        "\n",
        "\n",
        "            self.a_profit = actionAs*(self.todays_price-10)\n",
        "\n",
        "            self.b_profit = actionBs*(self.todays_price-10)\n",
        "\n",
        "            self.c_profit = actionCs*(self.todays_price-10)\n",
        "\n",
        "            self.state = np.array([actionAs, actionBs, actionCs, self.todays_price])\n",
        "\n",
        "            self.reward = self.a_profit, self.b_profit, self.c_profit\n",
        "\n",
        "            #print(\"end step state\", self.state)\n",
        "\n",
        "\n",
        "            return np.transpose(self.state), self.reward, done, {}\n",
        "\n",
        "     def reset(self):\n",
        "\n",
        "\n",
        "        self.steps_left = 2\n",
        "\n",
        "\n",
        "        self.state = np.array((0,0,0,0),dtype=np.float32)\n",
        "\n",
        "        #print(\"reset state\", self.state)\n",
        "\n",
        "        self.reward = 0\n",
        "\n",
        "        self.done = False\n",
        "\n",
        "        return np.transpose(self.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jG_-2Gamip1z"
      },
      "outputs": [],
      "source": [
        "#NN Models\n",
        "\n",
        "class ShrimpActor(nn.Module):\n",
        "    def __init__(self, obs_size, act_size):\n",
        "        super(ShrimpActor, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, act_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class ShrimpCritic(nn.Module):\n",
        "    def __init__(self, obs_size, act_size,\n",
        "                 n_atoms, v_min, v_max):\n",
        "        super(ShrimpCritic, self).__init__()\n",
        "\n",
        "        self.obs_net = nn.Sequential(\n",
        "            nn.Linear(obs_size, 400),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.out_net = nn.Sequential(\n",
        "            nn.Linear(400 + act_size, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, n_atoms)\n",
        "        )\n",
        "\n",
        "        delta = (v_max - v_min) / (n_atoms - 1)\n",
        "        self.register_buffer(\"supports\", torch.arange(\n",
        "            v_min, v_max + delta, delta))\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        obs = self.obs_net(x)\n",
        "        return self.out_net(torch.cat([obs, a], dim=1))\n",
        "\n",
        "    def distr_to_q(self, distr):\n",
        "        weights = F.softmax(distr, dim=1) * self.supports\n",
        "        res = weights.sum(dim=1)\n",
        "        return res.unsqueeze(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rqSv8Fa_jkH-"
      },
      "outputs": [],
      "source": [
        "class BaseAgent:\n",
        "    \"\"\"\n",
        "    Abstract Agent interface\n",
        "    \"\"\"\n",
        "    def initial_state(self):\n",
        "        \"\"\"\n",
        "        Should create initial empty state for the agent. It will be called for the start of the episode\n",
        "        :return: Anything agent want to remember\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def __call__(self, states, agent_states):\n",
        "        \"\"\"\n",
        "        Convert observations and states into actions to take\n",
        "        :param states: list of environment states to process\n",
        "        :param agent_states: list of states with the same length as observations\n",
        "        :return: tuple of actions, states\n",
        "        \"\"\"\n",
        "        assert isinstance(states, list)\n",
        "        assert isinstance(agent_states, list)\n",
        "        assert len(agent_states) == len(states)\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "def float32_preprocessor(states):\n",
        "    np_states = np.array(states, dtype=np.float32)\n",
        "    return torch.tensor(np_states)\n",
        "\n",
        "\n",
        "class AgentD4PG(BaseAgent):\n",
        "    \"\"\"\n",
        "    Agent implementing noisy agent\n",
        "    \"\"\"\n",
        "    def __init__(self, net, device=device, epsilon=EPSILON):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def __call__(self, states, agent_states):\n",
        "        states_v = float32_preprocessor(states)\n",
        "        states_v = states_v.to(self.device)\n",
        "        mu_v = self.net(states_v)\n",
        "        actions = mu_v.data.cpu().numpy()\n",
        "        actions += self.epsilon * np.random.normal(\n",
        "            size=actions.shape)\n",
        "        #actions = np.clip(actions, -1.7, 1.7)\n",
        "        return actions, agent_states\n",
        "\n",
        "\n",
        "class InsiderAgentD4PG(BaseAgent):\n",
        "    \"\"\"\n",
        "    Agent class to inject inside information\n",
        "    \"\"\"\n",
        "    def __init__(self, net, device=device, epsilon=EPSILON):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def __call__(self, states, agent_states):\n",
        "        states_v = float32_preprocessor(states)\n",
        "        states_v = states_v.to(self.device)\n",
        "        mu_v = self.net(states_v)\n",
        "\n",
        "\n",
        "        actions = mu_v.data.cpu().numpy()\n",
        "        actions += self.epsilon * np.random.normal(size=actions.shape)\n",
        "        #actions = np.clip(actions, -1.7, 1.7)\n",
        "\n",
        "        if random.uniform(0,1) <= .5:\n",
        "\n",
        "            for env in range(NUM_ENVS):\n",
        "\n",
        "              actions[env] = (45-50)/29.3\n",
        "\n",
        "        return actions, agent_states\n",
        "\n",
        "class TargetNet:\n",
        "    \"\"\"\n",
        "    Wrapper around model which provides copy of it instead of trained weights\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.target_model = copy.deepcopy(model)\n",
        "\n",
        "    def sync(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def alpha_sync(self, alpha):\n",
        "        \"\"\"\n",
        "        Blend params of target net with params from the model\n",
        "        :param alpha:\n",
        "        \"\"\"\n",
        "        assert isinstance(alpha, float)\n",
        "        assert 0.0 < alpha <= 1.0\n",
        "        state = self.model.state_dict()\n",
        "        tgt_state = self.target_model.state_dict()\n",
        "        for k, v in state.items():\n",
        "            tgt_state[k] = tgt_state[k] * alpha + (1 - alpha) * v\n",
        "        self.target_model.load_state_dict(tgt_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RKDv7JvUkMTf"
      },
      "outputs": [],
      "source": [
        "# one single experience step\n",
        "#Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
        "Experience = namedtuple('Experience', ['state', 'actionA', 'actionB', 'actionC', 'rewardA', 'rewardB', 'rewardC','done'])\n",
        "\n",
        "\n",
        "class ExperienceSourceMARL:\n",
        "    \"\"\"\n",
        "    Simple n-step experience source using single or multiple environments\n",
        "    Every experience contains n list of Experience entries\n",
        "    \"\"\"\n",
        "    def __init__(self, env, agentA, agentB, agentC, steps_count=2, steps_delta=1, vectorized=False):\n",
        "        \"\"\"\n",
        "        Create simple experience source\n",
        "        :param env: environment or list of environments to be used\n",
        "        :param agent: callable to convert batch of states into actions to take\n",
        "        :param steps_count: count of steps to track for every experience chain\n",
        "        :param steps_delta: how many steps to do between experience items\n",
        "        :param vectorized: support of vectorized envs from OpenAI universe\n",
        "        \"\"\"\n",
        "        assert isinstance(env, (gym.Env, list, tuple))\n",
        "        assert isinstance(agentA, BaseAgent)\n",
        "        assert isinstance(agentB, BaseAgent)\n",
        "        assert isinstance(agentC, BaseAgent)\n",
        "        assert isinstance(steps_count, int)\n",
        "        assert steps_count >= 1\n",
        "        assert isinstance(vectorized, bool)\n",
        "        if isinstance(env, (list, tuple)):\n",
        "            self.pool = env\n",
        "        else:\n",
        "            self.pool = [env]\n",
        "        self.agentA = agentA\n",
        "        self.agentB = agentB\n",
        "        self.agentC = agentC\n",
        "        self.steps_count = steps_count\n",
        "        self.steps_delta = steps_delta\n",
        "        self.total_rewardsA = []\n",
        "        self.total_rewardsB = []\n",
        "        self.total_rewardsC = []\n",
        "        self.total_steps = []\n",
        "        self.vectorized = vectorized\n",
        "\n",
        "    def __iter__(self):\n",
        "        states, agent_states, histories, cur_rewardsA, cur_rewardsB, cur_rewardsC, cur_steps = [], [], [], [], [], [], []\n",
        "        env_lens = []\n",
        "        for env in self.pool:\n",
        "            obs = env.reset()\n",
        "            print(type(obs))\n",
        "            # if the environment is vectorized, all it's output is lists of results.\n",
        "            # Details are here: https://github.com/openai/universe/blob/master/doc/env_semantics.rst\n",
        "            if self.vectorized:\n",
        "                obs_len = len(obs)\n",
        "                states.extend(obs)\n",
        "            else:\n",
        "                obs_len = 1\n",
        "                states.append(obs)\n",
        "            env_lens.append(obs_len)\n",
        "\n",
        "            for _ in range(obs_len):\n",
        "                histories.append(deque(maxlen=self.steps_count))\n",
        "                cur_rewardsA.append(0.0)\n",
        "                cur_rewardsB.append(0.0)\n",
        "                cur_rewardsC.append(0.0)\n",
        "                cur_steps.append(0)\n",
        "                #In shrimp game all agents share the same states\n",
        "                agent_states.append(self.agentA.initial_state())\n",
        "\n",
        "        iter_idx = 0\n",
        "        while True:\n",
        "            actionsA = [None] * len(states)\n",
        "            actionsB = [None] * len(states)\n",
        "            actionsC = [None] * len(states)\n",
        "            states_input = []\n",
        "            states_indices = []\n",
        "            for idx, state in enumerate(states):\n",
        "                if state is None:\n",
        "                    actionsA[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
        "                    actionsB[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
        "                    actionsC[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
        "                else:\n",
        "                    states_input.append(state)\n",
        "                    states_indices.append(idx)\n",
        "            if states_input:\n",
        "\n",
        "                states_actionsA, new_agent_states = self.agentA(states_input, agent_states)\n",
        "                states_actionsB = self.agentB(states_input, agent_states)[0]\n",
        "                states_actionsC = self.agentC(states_input, agent_states)[0]\n",
        "\n",
        "\n",
        "                #agent actions\n",
        "                for idx, (actionA, actionB, actionC) in enumerate(zip(states_actionsA, states_actionsB, states_actionsC)):\n",
        "                    g_idx = states_indices[idx]\n",
        "                    actionsA[g_idx] = actionA\n",
        "                    actionsB[g_idx] = actionB\n",
        "                    actionsC[g_idx] = actionC\n",
        "                    agent_states[g_idx] = new_agent_states[idx]\n",
        "                grouped_actionsA = _group_list(actionsA, env_lens)\n",
        "                grouped_actionsB = _group_list(actionsB, env_lens)\n",
        "                grouped_actionsC = _group_list(actionsC, env_lens)\n",
        "\n",
        "            global_ofs = 0\n",
        "            for env_idx, (env, action_A, action_B, action_C) in enumerate(zip(self.pool, grouped_actionsA, grouped_actionsB, grouped_actionsC)):\n",
        "                if self.vectorized:\n",
        "                    next_state_n, r_n, is_done_n, _ = env.step(action_A, action_B, action_C)\n",
        "                else:\n",
        "\n",
        "                    next_state, r, is_done, _ = env.step(action_A[0], action_B[0], action_C[0])\n",
        "                    next_state_n, r_n, is_done_n = [next_state], [r], [is_done]\n",
        "\n",
        "                for ofs, (actionA, actionB, actionC, next_state, r, is_done) in enumerate(zip(action_A, action_B, action_C, next_state_n, r_n, is_done_n)):\n",
        "                    idx = global_ofs + ofs\n",
        "                    state = states[idx]\n",
        "                    history = histories[idx]\n",
        "                    cur_rewardsA[idx] += r[0]\n",
        "                    cur_rewardsB[idx] += r[1]\n",
        "                    cur_rewardsC[idx] += r[2]\n",
        "                    cur_steps[idx] += 1\n",
        "                    if state is not None:\n",
        "                        history.append(Experience(state=state, actionA=actionA, actionB=actionB, actionC=actionC, rewardA=r[0], rewardB=r[1], rewardC=r[2], done=is_done))\n",
        "                    if len(history) == self.steps_count and iter_idx % self.steps_delta == 0:\n",
        "                        yield tuple(history)\n",
        "                    states[idx] = next_state\n",
        "                    if is_done:\n",
        "                        # in case of very short episode (shorter than our steps count), send gathered history\n",
        "                        if 0 < len(history) < self.steps_count:\n",
        "                            yield tuple(history)\n",
        "                        # generate tail of history\n",
        "                        while len(history) > 1:\n",
        "                            history.popleft()\n",
        "                            yield tuple(history)\n",
        "\n",
        "                        #Arthur rewards\n",
        "                        self.total_rewardsA.append(cur_rewardsA[idx])\n",
        "                        self.total_steps.append(cur_steps[idx])\n",
        "                        cur_rewardsA[idx] = 0.0\n",
        "                        cur_steps[idx] = 0\n",
        "\n",
        "                        #Bonnie rewards\n",
        "                        self.total_rewardsB.append(cur_rewardsB[idx])\n",
        "                        cur_rewardsB[idx] = 0.0\n",
        "\n",
        "                        #Clyde rewards\n",
        "                        self.total_rewardsC.append(cur_rewardsC[idx])\n",
        "                        cur_rewardsC[idx] = 0.0\n",
        "\n",
        "                        # vectorized envs are reset automatically\n",
        "                        states[idx] = env.reset() if not self.vectorized else None\n",
        "                        agent_states[idx] = self.agentA.initial_state()\n",
        "                        history.clear()\n",
        "                global_ofs += len(action_A)\n",
        "            iter_idx += 1\n",
        "\n",
        "    def pop_total_rewardsA(self):\n",
        "        r = self.total_rewardsA\n",
        "        if r:\n",
        "            self.total_rewardsA = []\n",
        "            self.total_steps = []\n",
        "        return r\n",
        "\n",
        "    def pop_total_rewardsB(self):\n",
        "        r = self.total_rewardsB\n",
        "        if r:\n",
        "            self.total_rewardsB = []\n",
        "            self.total_steps = []\n",
        "        return r\n",
        "\n",
        "    def pop_total_rewardsC(self):\n",
        "        r = self.total_rewardsC\n",
        "        if r:\n",
        "            self.total_rewardsC = []\n",
        "            self.total_steps = []\n",
        "        return r\n",
        "\n",
        "    def pop_rewards_stepsA(self):\n",
        "        res = list(zip(self.total_rewardsA, self.total_steps))\n",
        "        if res:\n",
        "            self.total_rewardsA, self.total_steps = [], []\n",
        "        return res\n",
        "\n",
        "    def pop_rewards_stepsB(self):\n",
        "        res = list(zip(self.total_rewardsB, self.total_steps))\n",
        "        if res:\n",
        "            self.total_rewardsB, self.total_steps = [], []\n",
        "        return res\n",
        "\n",
        "    def pop_rewards_stepsC(self):\n",
        "        res = list(zip(self.total_rewardsC, self.total_steps))\n",
        "        if res:\n",
        "            self.total_rewardsC, self.total_steps = [], []\n",
        "        return res\n",
        "\n",
        "\n",
        "\n",
        "def _group_list(items, lens):\n",
        "    \"\"\"\n",
        "    Unflat the list of items by lens\n",
        "    :param items: list of items\n",
        "    :param lens: list of integers\n",
        "    :return: list of list of items grouped by lengths\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    cur_ofs = 0\n",
        "    for g_len in lens:\n",
        "        res.append(items[cur_ofs:cur_ofs+g_len])\n",
        "        cur_ofs += g_len\n",
        "    return res\n",
        "\n",
        "\n",
        "# those entries are emitted from ExperienceSourceFirstLast. Reward is discounted over the trajectory piece\n",
        "ExperienceFirstLast = collections.namedtuple('ExperienceFirstLast', ('state', 'actionA', 'actionB', 'actionC', 'rewardA', 'rewardB', 'rewardC', 'last_state'))\n",
        "\n",
        "\n",
        "class ExperienceSourceFirstLastMARL(ExperienceSourceMARL):\n",
        "    \"\"\"\n",
        "    This is a wrapper around ExperienceSource to prevent storing full trajectory in replay buffer when we need\n",
        "    only first and last states. For every trajectory piece it calculates discounted reward and emits only first\n",
        "    and last states and action taken in the first state.\n",
        "    If we have partial trajectory at the end of episode, last_state will be None\n",
        "    \"\"\"\n",
        "    def __init__(self, env, agentA, agentB, agentC, gamma, steps_count=1, steps_delta=1, vectorized=False):\n",
        "        assert isinstance(gamma, float)\n",
        "        super(ExperienceSourceFirstLastMARL, self).__init__(env, agentA, agentB, agentC, steps_count+1, steps_delta, vectorized=vectorized)\n",
        "        self.gamma = gamma\n",
        "        self.steps = steps_count\n",
        "\n",
        "    def __iter__(self):\n",
        "        for exp in super(ExperienceSourceFirstLastMARL, self).__iter__():\n",
        "            if exp[-1].done and len(exp) <= self.steps:\n",
        "                last_state = None\n",
        "                elems = exp\n",
        "            else:\n",
        "                last_state = exp[-1].state\n",
        "                elems = exp[:-1]\n",
        "            total_rewardA = 0.0\n",
        "            total_rewardB = 0.0\n",
        "            total_rewardC = 0.0\n",
        "\n",
        "            for e in reversed(elems):\n",
        "                total_rewardA *= self.gamma\n",
        "                total_rewardA += e.rewardA\n",
        "\n",
        "                total_rewardB *= self.gamma\n",
        "                total_rewardB += e.rewardB\n",
        "\n",
        "                total_rewardC *= self.gamma\n",
        "                total_rewardC += e.rewardC\n",
        "            yield ExperienceFirstLast(state=exp[0].state, actionA=exp[0].actionA, actionB=exp[0].actionB, actionC=exp[0].actionC,\n",
        "                                      rewardA=total_rewardA, rewardB=total_rewardB, rewardC=total_rewardC,last_state=last_state)\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, experience_source, buffer_size):\n",
        "        assert isinstance(experience_source, (ExperienceSourceMARL, type(None)))\n",
        "        assert isinstance(buffer_size, int)\n",
        "        self.experience_source_iter = None if experience_source is None else iter(experience_source)\n",
        "        self.buffer = []\n",
        "        self.capacity = buffer_size\n",
        "        self.pos = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.buffer)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Get one random batch from experience replay\n",
        "        TODO: implement sampling order policy\n",
        "        :param batch_size:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if len(self.buffer) <= batch_size:\n",
        "            return self.buffer\n",
        "        # Warning: replace=False makes random.choice O(n)\n",
        "        keys = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
        "        return [self.buffer[key] for key in keys]\n",
        "\n",
        "    def _add(self, sample):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(sample)\n",
        "        else:\n",
        "            self.buffer[self.pos] = sample\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def populate(self, samples):\n",
        "        \"\"\"\n",
        "        Populates samples into the buffer\n",
        "        :param samples: how many samples to populate\n",
        "        \"\"\"\n",
        "        for _ in range(samples):\n",
        "            entry = next(self.experience_source_iter)\n",
        "            self._add(entry)\n",
        "\n",
        "class PrioritizedReplayBuffer(ExperienceReplayBuffer):\n",
        "    def __init__(self, experience_source, buffer_size, alpha):\n",
        "        super(PrioritizedReplayBuffer, self).__init__(experience_source, buffer_size)\n",
        "        assert alpha > 0\n",
        "        self._alpha = alpha\n",
        "\n",
        "        it_capacity = 1\n",
        "        while it_capacity < buffer_size:\n",
        "            it_capacity *= 2\n",
        "\n",
        "        self._it_sum = utils.SumSegmentTree(it_capacity)\n",
        "        self._it_min = utils.MinSegmentTree(it_capacity)\n",
        "        self._max_priority = 1.0\n",
        "\n",
        "    def _add(self, *args, **kwargs):\n",
        "        idx = self.pos\n",
        "        super()._add(*args, **kwargs)\n",
        "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
        "        self._it_min[idx] = self._max_priority ** self._alpha\n",
        "\n",
        "    def _sample_proportional(self, batch_size):\n",
        "        res = []\n",
        "        for _ in range(batch_size):\n",
        "            mass = random.random() * self._it_sum.sum(0, len(self) - 1)\n",
        "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
        "            res.append(idx)\n",
        "        return res\n",
        "\n",
        "    def sample(self, batch_size, beta):\n",
        "        assert beta > 0\n",
        "\n",
        "        idxes = self._sample_proportional(batch_size)\n",
        "\n",
        "        weights = []\n",
        "        p_min = self._it_min.min() / self._it_sum.sum()\n",
        "        max_weight = (p_min * len(self)) ** (-beta)\n",
        "\n",
        "        for idx in idxes:\n",
        "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
        "            weight = (p_sample * len(self)) ** (-beta)\n",
        "            weights.append(weight / max_weight)\n",
        "        weights = np.array(weights, dtype=np.float32)\n",
        "        samples = [self.buffer[idx] for idx in idxes]\n",
        "        return samples, idxes, weights\n",
        "\n",
        "    def update_priorities(self, idxes, priorities):\n",
        "        assert len(idxes) == len(priorities)\n",
        "        for idx, priority in zip(idxes, priorities):\n",
        "            assert priority > 0\n",
        "            assert 0 <= idx < len(self)\n",
        "            self._it_sum[idx] = priority ** self._alpha\n",
        "            self._it_min[idx] = priority ** self._alpha\n",
        "\n",
        "            self._max_priority = max(self._max_priority, priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VzdI_QR6QCli"
      },
      "outputs": [],
      "source": [
        "def test_net(netA, netB, netC, env, count=NUM_ENVS, device=device):\n",
        "    rewardsA = 0.0\n",
        "    rewardsB = 0.0\n",
        "    rewardsC = 0.0\n",
        "    steps = 0\n",
        "    for _ in range(count):\n",
        "        obs = env.reset()\n",
        "        while True:\n",
        "            obs_v = float32_preprocessor([obs]).to(device)\n",
        "            mu_vA = netA(obs_v)\n",
        "            mu_vB = netB(obs_v)\n",
        "            mu_vC = netC(obs_v)\n",
        "            actionA = mu_vA.squeeze(dim=0).data.cpu().numpy()\n",
        "            actionB = mu_vB.squeeze(dim=0).data.cpu().numpy()\n",
        "            actionC = mu_vC.squeeze(dim=0).data.cpu().numpy()\n",
        "            #actionA = np.clip(actionA, -1.7, 1.7)\n",
        "            #actionB = np.clip(actionB, -1.7, 1.7)\n",
        "            #actionC = np.clip(actionC, -1.7, 1.7)\n",
        "            obs, reward, done, _ = env.step(actionA, actionB, actionC)\n",
        "            rewardsA += reward[0]\n",
        "            rewardsB += reward[1]\n",
        "            rewardsC += reward[2]\n",
        "            steps += 1\n",
        "            if done:\n",
        "                break\n",
        "    return rewardsA / count, rewardsB / count, rewardsC / count, steps / count\n",
        "\n",
        "\n",
        "def unpack_batch_ddqn(batch, device=device):\n",
        "    states, actionsA, actionsB, actionsC, rewardsA, rewardsB, rewardsC, dones, last_states = [], [], [], [], [], [], [], [], []\n",
        "    for exp in batch:\n",
        "        states.append(exp.state)\n",
        "        actionsA.append(exp.actionA)\n",
        "        actionsB.append(exp.actionB)\n",
        "        actionsC.append(exp.actionC)\n",
        "        rewardsA.append(exp.rewardA)\n",
        "        rewardsB.append(exp.rewardB)\n",
        "        rewardsC.append(exp.rewardC)\n",
        "        dones.append(exp.last_state is None)\n",
        "        if exp.last_state is None:\n",
        "            last_states.append(exp.state)\n",
        "        else:\n",
        "            last_states.append(exp.last_state)\n",
        "    states_v = float32_preprocessor(states).to(device)\n",
        "    actions_vA = float32_preprocessor(actionsA).to(device)\n",
        "    rewards_vA = float32_preprocessor(rewardsA).to(device)\n",
        "    actions_vB = float32_preprocessor(actionsB).to(device)\n",
        "    rewards_vB = float32_preprocessor(rewardsB).to(device)\n",
        "    actions_vC = float32_preprocessor(actionsC).to(device)\n",
        "    rewards_vC = float32_preprocessor(rewardsC).to(device)\n",
        "    last_states_v = float32_preprocessor(last_states).to(device)\n",
        "    dones_t = torch.BoolTensor(dones).to(device)\n",
        "    return states_v, actions_vA, actions_vB, actions_vC, rewards_vA, rewards_vB, rewards_vC, dones_t, last_states_v\n",
        "\n",
        "def distr_projection(next_distr_v, rewards_v, dones_mask_t,\n",
        "                     gamma, device=\"cpu\"):\n",
        "    next_distr = next_distr_v.data.cpu().numpy()\n",
        "    rewards = rewards_v.data.cpu().numpy()\n",
        "    dones_mask = dones_mask_t.cpu().numpy().astype(bool)\n",
        "    batch_size = len(rewards)\n",
        "    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)\n",
        "\n",
        "    for atom in range(N_ATOMS):\n",
        "        tz_j = np.minimum(VMAX, np.maximum(\n",
        "            VMIN, rewards + (VMIN + atom * DELTA_Z) * gamma))\n",
        "        b_j = (tz_j - VMIN) / DELTA_Z\n",
        "        l = np.floor(b_j).astype(np.int64)\n",
        "        u = np.ceil(b_j).astype(np.int64)\n",
        "        eq_mask = u == l\n",
        "        proj_distr[eq_mask, l[eq_mask]] += \\\n",
        "            next_distr[eq_mask, atom]\n",
        "        ne_mask = u != l\n",
        "        proj_distr[ne_mask, l[ne_mask]] += \\\n",
        "            next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n",
        "        proj_distr[ne_mask, u[ne_mask]] += \\\n",
        "            next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n",
        "\n",
        "    if dones_mask.any():\n",
        "        proj_distr[dones_mask] = 0.0\n",
        "        tz_j = np.minimum(VMAX, np.maximum(\n",
        "            VMIN, rewards[dones_mask]))\n",
        "        b_j = (tz_j - VMIN) / DELTA_Z\n",
        "        l = np.floor(b_j).astype(np.int64)\n",
        "        u = np.ceil(b_j).astype(np.int64)\n",
        "        eq_mask = u == l\n",
        "        eq_dones = dones_mask.copy()\n",
        "        eq_dones[dones_mask] = eq_mask\n",
        "        if eq_dones.any():\n",
        "            proj_distr[eq_dones, l[eq_mask]] = 1.0\n",
        "        ne_mask = u != l\n",
        "        ne_dones = dones_mask.copy()\n",
        "        ne_dones[dones_mask] = ne_mask\n",
        "        if ne_dones.any():\n",
        "            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n",
        "            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n",
        "    return torch.FloatTensor(proj_distr).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JJd4iRswxh12"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    save_path = os.path.join(\"saves\", \"a2c-\" + \"ShrimpNet\")\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    envs = [ShrimpEnv() for _ in range(NUM_ENVS)]\n",
        "    test_env = ShrimpEnv()\n",
        "    writer = SummaryWriter(comment=\"ShrimpNet\")\n",
        "\n",
        "    netA_actor = ShrimpActor(envs[0].observation_space.shape[0], envs[0].action_space.n).to(device)\n",
        "    netA_critic = ShrimpCritic(envs[0].observation_space.shape[0], envs[0].action_space.n, N_ATOMS, VMIN, VMAX).to(device)\n",
        "    #print(netA_actor)\n",
        "    #print(netA_critic)\n",
        "    tgt_netA_actor = TargetNet(netA_actor)\n",
        "    tgt_netA_critic = TargetNet(netA_critic)\n",
        "\n",
        "    netB_actor = ShrimpActor(envs[0].observation_space.shape[0], envs[0].action_space.n).to(device)\n",
        "    netB_critic = ShrimpCritic(envs[0].observation_space.shape[0], envs[0].action_space.n, N_ATOMS, VMIN, VMAX).to(device)\n",
        "\n",
        "    tgt_netB_actor = TargetNet(netB_actor)\n",
        "    tgt_netB_critic = TargetNet(netB_critic)\n",
        "\n",
        "    netC_actor = ShrimpActor(envs[0].observation_space.shape[0], envs[0].action_space.n).to(device)\n",
        "    netC_critic = ShrimpCritic(envs[0].observation_space.shape[0], envs[0].action_space.n, N_ATOMS, VMIN, VMAX).to(device)\n",
        "\n",
        "    tgt_netC_actor = TargetNet(netC_actor)\n",
        "    tgt_netC_critic = TargetNet(netC_critic)\n",
        "\n",
        "    agentA = AgentD4PG(netA_actor, device=device)\n",
        "\n",
        "    agentB = AgentD4PG(netB_actor, device=device)\n",
        "\n",
        "    agentC = AgentD4PG(netC_actor, device=device)\n",
        "\n",
        "    exp_source = ExperienceSourceFirstLastMARL(envs, agentA, agentB, agentC, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
        "    buffer = ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)\n",
        "\n",
        "    optimizerA_actor = optim.Adam(netA_actor.parameters(), lr=LEARNING_RATE_ACTOR)\n",
        "    optimizerA_critic = optim.Adam(netA_critic.parameters(), lr=LEARNING_RATE_CRITIC)\n",
        "\n",
        "    optimizerB_actor = optim.Adam(netB_actor.parameters(), lr=LEARNING_RATE_ACTOR)\n",
        "    optimizerB_critic = optim.Adam(netB_critic.parameters(), lr=LEARNING_RATE_CRITIC)\n",
        "\n",
        "    optimizerC_actor = optim.Adam(netC_actor.parameters(), lr=LEARNING_RATE_ACTOR)\n",
        "    optimizerC_critic = optim.Adam(netC_critic.parameters(), lr=LEARNING_RATE_CRITIC)\n",
        "\n",
        "    frame_idx = 0\n",
        "    best_rewardA = None\n",
        "    best_rewardB = None\n",
        "    best_rewardC = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySmIU_HG0xbT"
      },
      "outputs": [],
      "source": [
        "with RewardTracker(writer, STOP_REWARD) as tracker:\n",
        "        with TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
        "            while True:\n",
        "                frame_idx += 1\n",
        "                buffer.populate(1)\n",
        "\n",
        "                rewards_stepsA = exp_source.pop_rewards_stepsA()\n",
        "                rewards_stepsB = exp_source.pop_rewards_stepsB()\n",
        "                rewards_stepsC = exp_source.pop_rewards_stepsC()\n",
        "\n",
        "                if rewards_stepsA:\n",
        "                    rewardsA, steps = zip(*rewards_stepsA)\n",
        "                    tb_tracker.track(\"episode_steps\", steps[0], frame_idx)\n",
        "                    tracker.rewardA(rewardsA[0], frame_idx)\n",
        "\n",
        "                elif rewards_stepsB:\n",
        "                    rewardsB, steps = zip(*rewards_stepsB)\n",
        "                    tb_tracker.track(\"episode_steps\", steps[0], frame_idx)\n",
        "                    tracker.rewardB(rewardsB[0], frame_idx)\n",
        "\n",
        "                elif rewards_stepsC:\n",
        "                    rewardsC, steps = zip(*rewards_stepsC)\n",
        "                    tb_tracker.track(\"episode_steps\", steps[0], frame_idx)\n",
        "                    tracker.rewardC(rewardsC[0], frame_idx)\n",
        "\n",
        "                if len(buffer) < REPLAY_INITIAL:\n",
        "                    continue\n",
        "\n",
        "                batch = buffer.sample(BATCH_SIZE)\n",
        "                states_v, actions_vA, actions_vB, actions_vC, rewards_vA, rewards_vB, rewards_vC, dones_mask, last_states_v = unpack_batch_ddqn(batch, device)\n",
        "\n",
        "                #print(\"STATES_V\", len(states_v), states_v)\n",
        "                #print(\"ACTIONS_V\", len(actions_vA), actions_vA)\n",
        "                #print(\"REWARDS_V\", len(rewards_vA),rewards_vA)\n",
        "                #print(\"LAST STATES_V\", len(last_states_v), last_states_v)\n",
        "\n",
        "                actions = []\n",
        "                actions = ((actions_vA*29.3+50), (actions_vB*29.3+50), (actions_vC*29.3+50))\n",
        "                mean_actions = (torch.mean(actions_vA*29.3+50)), (torch.mean(actions_vB*29.3+50)), (torch.mean(actions_vC*29.3+50))\n",
        "\n",
        "\n",
        "                # train Arthur critic\n",
        "                optimizerA_critic.zero_grad()\n",
        "                crt_distr_vA = netA_critic(states_v, actions_vA)\n",
        "                last_act_vA = tgt_netA_actor.target_model(last_states_v)\n",
        "                last_distr_vA = F.softmax(tgt_netA_critic.target_model(last_states_v, last_act_vA), dim=1)\n",
        "                proj_distr_vA = distr_projection(last_distr_vA, rewards_vA, dones_mask, gamma=GAMMA**REWARD_STEPS, device=device)\n",
        "                prob_dist_vA = -F.log_softmax(crt_distr_vA, dim=1) * proj_distr_vA\n",
        "                critic_loss_vA = prob_dist_vA.sum(dim=1).mean()\n",
        "                critic_loss_vA.backward()\n",
        "                optimizerA_critic.step()\n",
        "                tb_tracker.track(\"Arthur's loss_critic\", critic_loss_vA, frame_idx)\n",
        "\n",
        "                # train Arthur actor\n",
        "                optimizerA_actor.zero_grad()\n",
        "                cur_actions_vA = netA_actor(states_v)\n",
        "                crt_distr_vA = netA_critic(states_v, cur_actions_vA)\n",
        "                actor_loss_vA = -netA_critic.distr_to_q(crt_distr_vA)\n",
        "                actor_loss_vA = actor_loss_vA.mean()\n",
        "                actor_loss_vA.backward()\n",
        "                optimizerA_actor.step()\n",
        "                tb_tracker.track(\"Arthur's loss_actor\", actor_loss_vA,\n",
        "                                 frame_idx)\n",
        "\n",
        "                tgt_netA_actor.alpha_sync(alpha=1 - 1e-3)\n",
        "                tgt_netA_critic.alpha_sync(alpha=1 - 1e-3)\n",
        "\n",
        "                # train Bonnie critic\n",
        "                optimizerB_critic.zero_grad()\n",
        "                crt_distr_vB = netB_critic(states_v, actions_vB)\n",
        "                last_act_vB = tgt_netB_actor.target_model(last_states_v)\n",
        "                last_distr_vB = F.softmax(tgt_netB_critic.target_model(last_states_v, last_act_vB), dim=1)\n",
        "                proj_distr_vB = distr_projection(last_distr_vB, rewards_vB, dones_mask, gamma=GAMMA**REWARD_STEPS, device=device)\n",
        "                prob_dist_vB = -F.log_softmax(crt_distr_vB, dim=1) * proj_distr_vB\n",
        "                critic_loss_vB = prob_dist_vB.sum(dim=1).mean()\n",
        "                critic_loss_vB.backward()\n",
        "                optimizerB_critic.step()\n",
        "                tb_tracker.track(\"Bonnie's loss_critic\", critic_loss_vB, frame_idx)\n",
        "\n",
        "                # train Bonnie actor\n",
        "                optimizerB_actor.zero_grad()\n",
        "                cur_actions_vB = netB_actor(states_v)\n",
        "                crt_distr_vB = netB_critic(states_v, cur_actions_vB)\n",
        "                actor_loss_vB = -netB_critic.distr_to_q(crt_distr_vB)\n",
        "                actor_loss_vB = actor_loss_vB.mean()\n",
        "                actor_loss_vB.backward()\n",
        "                optimizerB_actor.step()\n",
        "                tb_tracker.track(\"Bonnie's loss_actor\", actor_loss_vB,\n",
        "                                 frame_idx)\n",
        "\n",
        "                tgt_netB_actor.alpha_sync(alpha=1 - 1e-3)\n",
        "                tgt_netB_critic.alpha_sync(alpha=1 - 1e-3)\n",
        "\n",
        "                # train Clyde critic\n",
        "                optimizerC_critic.zero_grad()\n",
        "                crt_distr_vC = netC_critic(states_v, actions_vC)\n",
        "                last_act_vC = tgt_netC_actor.target_model(last_states_v)\n",
        "                last_distr_vC = F.softmax(tgt_netC_critic.target_model(last_states_v, last_act_vC), dim=1)\n",
        "                proj_distr_vC = distr_projection(last_distr_vC, rewards_vC, dones_mask, gamma=GAMMA**REWARD_STEPS, device=device)\n",
        "                prob_dist_vC = -F.log_softmax(crt_distr_vC, dim=1) * proj_distr_vC\n",
        "                critic_loss_vC = prob_dist_vC.sum(dim=1).mean()\n",
        "                critic_loss_vC.backward()\n",
        "                optimizerC_critic.step()\n",
        "                tb_tracker.track(\"Clyde's loss_critic\", critic_loss_vC, frame_idx)\n",
        "\n",
        "                # train Clyde actor\n",
        "                optimizerC_actor.zero_grad()\n",
        "                cur_actions_vC = netC_actor(states_v)\n",
        "                crt_distr_vC = netC_critic(states_v, cur_actions_vC)\n",
        "                actor_loss_vC = -netC_critic.distr_to_q(crt_distr_vC)\n",
        "                actor_loss_vC = actor_loss_vC.mean()\n",
        "                actor_loss_vC.backward()\n",
        "                optimizerC_actor.step()\n",
        "                tb_tracker.track(\"Clydes's loss_actor\", actor_loss_vC,\n",
        "                                 frame_idx)\n",
        "\n",
        "                tgt_netC_actor.alpha_sync(alpha=1 - 1e-3)\n",
        "                tgt_netC_critic.alpha_sync(alpha=1 - 1e-3)\n",
        "\n",
        "                if frame_idx % TEST_ITERS == 0:\n",
        "                    ts = time.time()\n",
        "                    rewardsA, rewardsB, rewardsC, steps = test_net(netA_actor, netB_actor, netC_actor, test_env, device=device)\n",
        "                    community = (rewardsA + rewardsB + rewardsC)/3\n",
        "                    #print(\"Mean rewards\", torch.mean(rewards_vA), torch.mean(rewards_vB), torch.mean(rewards_vC))\n",
        "                    #print(\"actions\", actions)\n",
        "                    #print(\"mean actions\", mean_actions)\n",
        "                    #print(\"Test done in %.2f sec, Arthur's reward %.3f, Bonnie's reward %.3f, Clyde's reward %.3f, Community reward %.3f, steps %d\" % (\n",
        "                        #time.time() - ts, rewardsA, rewardsB, rewardsC, community, steps))\n",
        "                    writer.add_scalar(\"Arthur's test_reward\", rewardsA, frame_idx)\n",
        "                    writer.add_scalar(\"Bonnie's test_reward\", rewardsB, frame_idx)\n",
        "                    writer.add_scalar(\"Clyde's test_reward\", rewardsC, frame_idx)\n",
        "                    writer.add_scalar(\"Community Average Rewards\", community, frame_idx)\n",
        "                    writer.add_scalar(\"test_steps\", steps, frame_idx)\n",
        "                    if best_rewardA is None or best_rewardA < rewardsA:\n",
        "                        if best_rewardA is not None:\n",
        "                            print(\"Arthur's Best reward updated: %.3f -> %.3f\" % (best_rewardA, rewardsA))\n",
        "                            name = \"Arthur's best_%+.3f_%d.dat\" % (rewardsA, frame_idx)\n",
        "                            fname = os.path.join(save_path, name)\n",
        "                            torch.save(netA_actor.state_dict(), fname)\n",
        "                        best_rewardA = rewardsA\n",
        "\n",
        "                    elif best_rewardB is None or best_rewardB < rewardsB:\n",
        "                        if best_rewardB is not None:\n",
        "                            print(\"Bonnie's Best reward updated: %.3f -> %.3f\" % (best_rewardB, rewardsB))\n",
        "                            name = \"Bonnie's best_%+.3f_%d.dat\" % (rewardsB, frame_idx)\n",
        "                            fname = os.path.join(save_path, name)\n",
        "                            torch.save(netB_actor.state_dict(), fname)\n",
        "                        best_rewardB = rewardsB\n",
        "\n",
        "                    elif best_rewardC is None or best_rewardC < rewardsC:\n",
        "                        if best_rewardC is not None:\n",
        "                            print(\"Clyde's Best reward updated: %.3f -> %.3f\" % (best_rewardC, rewardsC))\n",
        "                            name = \"Clyde's best_%+.3f_%d.dat\" % (rewardsC, frame_idx)\n",
        "                            fname = os.path.join(save_path, name)\n",
        "                            torch.save(netC_actor.state_dict(), fname)\n",
        "                        best_rewardC = rewardsC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfmvRiTqdMp1"
      },
      "outputs": [],
      "source": [
        "actions = []\n",
        "actions = (actions_vA[0:10]*29.3+50), (actions_vB[0:10]*29.3+50), (actions_vC[0:10]*29.3+50)\n",
        "print(actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVS2k1ubRNRU"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}